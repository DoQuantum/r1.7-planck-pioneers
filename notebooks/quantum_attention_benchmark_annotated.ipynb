{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f79eb5",
   "metadata": {},
   "source": [
    "# Quantum-Inspired Sparse Attention - Benchmark Notebook\n",
    "This notebook compares classical and quantum-inspired attention mechanisms on a toy classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd57d2",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Load required PyTorch modules and utilities for model building and benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6663c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.7.1 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from torch==2.7.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from torch==2.7.1) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from torch==2.7.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from torch==2.7.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from torch==2.7.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from torch==2.7.1) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from torch==2.7.1) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from jinja2->torch==2.7.1) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Run once to install PyTorch in your environment\n",
    "%pip install torch==2.7.1 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2cf57a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4ca8b",
   "metadata": {},
   "source": [
    "## Classical Multi-Head Attention\n",
    "Defines the standard multi-head attention block used in Transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0e3a619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical Multi-Head Attention Block\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model is not divisible by num_heads\"\n",
    "\n",
    "        self.d_k = d_model // num_heads # Dimension of each head\n",
    "        self.w_q = nn.Linear(d_model, d_model) # Linear layer for queries\n",
    "        self.w_k = nn.Linear(d_model, d_model) # Linear layer for keys\n",
    "        self.w_v = nn.Linear(d_model, d_model) # Linear layer for values\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model) # Linear layer for output\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k) # Scaled dot-product attention\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, float('-inf')) # Apply mask to attention scores\n",
    "        attention_scores = attention_scores.softmax(dim = -1) # Softmax to get attention weights\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        # 1st val in tuple - Compute the weighted sum of values based on attention scores\n",
    "        # 2nd val in tuple - visualizing the attention weights\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # Linear transformation for queries\n",
    "        key = self.w_k(k) # Linear transformation for keys\n",
    "        value = self.w_v(v) # Linear transformation for values\n",
    "\n",
    "        # Reshape and transpose the tensors to prepare for multi-head attention\n",
    "        query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        x, attn = self.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Reshape the output tensor back to the original shape\n",
    "        x = x.transpose(1, 2).contiguous().view(x.size(0), x.size(2), self.num_heads * self.d_k)\n",
    "        \n",
    "        # Apply the final linear transformation to the output\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e957328",
   "metadata": {},
   "source": [
    "## Create Simple Sample Dataset\n",
    "Generates 8 random samples with 4 tokens each and binary labels for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "36f0b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy dataset: 8 samples, 4 tokens each, 8-dim embeddings\n",
    "X = torch.rand(8, 4, 8)  # batch_size=8, seq_len=4, d_model=8\n",
    "y = torch.randint(0, 2, (8,))  # Binary labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb18e0e",
   "metadata": {},
   "source": [
    "## Classical Attention Model\n",
    "Defines a classifier using the classical multi-head attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "da9788b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple classifier using classical attention\n",
    "class ClassicalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttentionBlock(d_model=8, num_heads=2, dropout=0.1)\n",
    "        self.fc = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.attn(x, x, x, None)\n",
    "        out = out.mean(dim=1)  # Global average pooling over sequence\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e0304",
   "metadata": {},
   "source": [
    "## Quantum-Inspired Attention Function\n",
    "Simulates quantum behavior by using random softmax weights instead of dot-product scores (I didn't use a companies simulator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1e85f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated quantum-inspired attention (randomized weights)\n",
    "def quantum_inspired_attention(query, key, value):\n",
    "    B, H, T, D = query.shape\n",
    "    scores = torch.rand(B, H, T, T)\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    return weights @ value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dfd058",
   "metadata": {},
   "source": [
    "## Quantum-Inspired Model\n",
    "Builds a classifier that uses the quantum-inspired attention for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0d87a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum-inspired classifier\n",
    "class QuantumInspiredClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w_q = nn.Linear(8, 8)\n",
    "        self.w_k = nn.Linear(8, 8)\n",
    "        self.w_v = nn.Linear(8, 8)\n",
    "        self.fc = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.w_q(x).view(x.size(0), 2, 4, 4).transpose(1, 2)\n",
    "        k = self.w_k(x).view(x.size(0), 2, 4, 4).transpose(1, 2)\n",
    "        v = self.w_v(x).view(x.size(0), 2, 4, 4).transpose(1, 2)\n",
    "        out = quantum_inspired_attention(q, k, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(x.size(0), 4, 8)\n",
    "        out = out.mean(dim=1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8c3e4",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "Defines a function to compute accuracy and runtime of a given model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01c8043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, X, y):\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        accuracy = (preds == y).float().mean().item()\n",
    "    elapsed = time.time() - start\n",
    "    return accuracy, elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e666d",
   "metadata": {},
   "source": [
    "## Run and Compare Models\n",
    "Evaluates both models and prints out their accuracy and inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5cc6502b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical Accuracy: 25.0% | Time: 0.0010s\n",
      "Quantum-Inspired Accuracy: 25.0% | Time: 0.0005s\n"
     ]
    }
   ],
   "source": [
    "# Initialize and evaluate both models\n",
    "classical_model = ClassicalClassifier()\n",
    "quantum_model = QuantumInspiredClassifier()\n",
    "\n",
    "acc_classical, time_classical = evaluate(classical_model, X, y)\n",
    "acc_quantum, time_quantum = evaluate(quantum_model, X, y)\n",
    "\n",
    "print(f\"Classical Accuracy: {acc_classical*100:.1f}% | Time: {time_classical:.4f}s\")\n",
    "print(f\"Quantum-Inspired Accuracy: {acc_quantum*100:.1f}% | Time: {time_quantum:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3d008",
   "metadata": {},
   "source": [
    "# Self-Attention Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d2cf7c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.53.2 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from transformers==4.53.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from transformers==4.53.2) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from transformers==4.53.2) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from transformers==4.53.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from transformers==4.53.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from transformers==4.53.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from transformers==4.53.2) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from transformers==4.53.2) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from transformers==4.53.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from transformers==4.53.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (4.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from tqdm>=4.27->transformers==4.53.2) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from requests->transformers==4.53.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from requests->transformers==4.53.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from requests->transformers==4.53.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\evren\\anaconda3\\envs\\planckteam\\lib\\site-packages (from requests->transformers==4.53.2) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.53.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33419c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2296, 2305, 1045, 4682, 1999, 2793, 1012, 102, 0], [101, 1996, 26849, 6087, 6039, 2026, 2132, 1012, 1037, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "sample_text = \"Every night I lie in bed.\"\n",
    "sample_text2 = \"The brightest colors fill my head. A\"\n",
    "tokenizer([sample_text, sample_text2], padding=True)\n",
    "\n",
    "# Output of the tokenizer\n",
    "# {'input_ids': [[101, 2296, 2305, 1045, 4682, 1999, 2793, 1012, 102, 0],\n",
    "#                [101, 1996, 26849, 6087, 6039, 2026, 2132, 1012, 1037, 102]],\n",
    "#  'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "#                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0addef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3111e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c3d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560cb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9be85e26",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planckteam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
