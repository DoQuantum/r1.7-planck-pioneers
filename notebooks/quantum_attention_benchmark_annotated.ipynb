{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f79eb5",
   "metadata": {},
   "source": [
    "# Quantum-Inspired Sparse Attention - Benchmark Notebook\n",
    "This notebook compares classical and quantum-inspired attention mechanisms on a toy classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd57d2",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Load required PyTorch modules and utilities for model building and benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf57a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4ca8b",
   "metadata": {},
   "source": [
    "## Classical Multi-Head Attention\n",
    "Defines the standard multi-head attention block used in Transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical Multi-Head Attention Block\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model is not divisible by num_heads\"\n",
    "\n",
    "        self.d_k = d_model // num_heads # Dimension of each head\n",
    "        self.w_q = nn.Linear(d_model, d_model) # Linear layer for queries\n",
    "        self.w_k = nn.Linear(d_model, d_model) # Linear layer for keys\n",
    "        self.w_v = nn.Linear(d_model, d_model) # Linear layer for values\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model) # Linear layer for output\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k) # Scaled dot-product attention\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, float('-inf')) # Apply mask to attention scores\n",
    "        attention_scores = attention_scores.softmax(dim = -1) # Softmax to get attention weights\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        # 1st val in tuple - Compute the weighted sum of values based on attention scores\n",
    "        # 2nd val in tuple - visualizing the attention weights\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # Linear transformation for queries\n",
    "        key = self.w_k(k) # Linear transformation for keys\n",
    "        value = self.w_v(v) # Linear transformation for values\n",
    "\n",
    "        # Reshape and transpose the tensors to prepare for multi-head attention\n",
    "        query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        x, attn = self.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Reshape the output tensor back to the original shape\n",
    "        x = x.transpose(1, 2).contiguous().view(x.size(0), x.size(2), self.num_heads * self.d_k)\n",
    "        \n",
    "        # Apply the final linear transformation to the output\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e957328",
   "metadata": {},
   "source": [
    "## Create Simple Sample Dataset\n",
    "Generates 8 random samples with 4 tokens each and binary labels for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f0b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy dataset: 8 samples, 4 tokens each, 8-dim embeddings\n",
    "X = torch.rand(8, 4, 8)  # batch_size=8, seq_len=4, d_model=8\n",
    "y = torch.randint(0, 2, (8,))  # Binary labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb18e0e",
   "metadata": {},
   "source": [
    "## Classical Attention Model\n",
    "Defines a classifier using the classical multi-head attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9788b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple classifier using classical attention\n",
    "class ClassicalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttentionBlock(d_model=8, num_heads=2, dropout=0.1)\n",
    "        self.fc = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.attn(x, x, x, None)\n",
    "        out = out.mean(dim=1)  # Global average pooling over sequence\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e0304",
   "metadata": {},
   "source": [
    "## Quantum-Inspired Attention Function\n",
    "Simulates quantum behavior by using random softmax weights instead of dot-product scores (I didn't use a companies simulator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e85f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated quantum-inspired attention (randomized weights)\n",
    "def quantum_inspired_attention(query, key, value):\n",
    "    B, H, T, D = query.shape\n",
    "    scores = torch.rand(B, H, T, T)\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    return weights @ value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dfd058",
   "metadata": {},
   "source": [
    "## Quantum-Inspired Model\n",
    "Builds a classifier that uses the quantum-inspired attention for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum-inspired classifier\n",
    "class QuantumInspiredClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w_q = nn.Linear(8, 8)\n",
    "        self.w_k = nn.Linear(8, 8)\n",
    "        self.w_v = nn.Linear(8, 8)\n",
    "        self.fc = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.w_q(x).view(x.size(0), 2, 4, 4).transpose(1, 2)\n",
    "        k = self.w_k(x).view(x.size(0), 2, 4, 4).transpose(1, 2)\n",
    "        v = self.w_v(x).view(x.size(0), 2, 4, 4).transpose(1, 2)\n",
    "        out = quantum_inspired_attention(q, k, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(x.size(0), 4, 8)\n",
    "        out = out.mean(dim=1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8c3e4",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "Defines a function to compute accuracy and runtime of a given model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c8043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, X, y):\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        accuracy = (preds == y).float().mean().item()\n",
    "    elapsed = time.time() - start\n",
    "    return accuracy, elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e666d",
   "metadata": {},
   "source": [
    "## Run and Compare Models\n",
    "Evaluates both models and prints out their accuracy and inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6502b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical Accuracy: 37.5% | Time: 0.0025s\n",
      "Quantum-Inspired Accuracy: 37.5% | Time: 0.0021s\n"
     ]
    }
   ],
   "source": [
    "# Initialize and evaluate both models\n",
    "classical_model = ClassicalClassifier()\n",
    "quantum_model = QuantumInspiredClassifier()\n",
    "\n",
    "acc_classical, time_classical = evaluate(classical_model, X, y)\n",
    "acc_quantum, time_quantum = evaluate(quantum_model, X, y)\n",
    "\n",
    "print(f\"Classical Accuracy: {acc_classical*100:.1f}% | Time: {time_classical:.4f}s\")\n",
    "print(f\"Quantum-Inspired Accuracy: {acc_quantum*100:.1f}% | Time: {time_quantum:.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planckteam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
